{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514cde24-8c4e-4d25-a2ef-370135ad84dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How does a neural net really work? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d45677-777b-4c48-9521-18944863ff92",
   "metadata": {},
   "source": [
    "Exploring a basic neural net to get an intuition of how it works. \n",
    "\n",
    "This current notebook adapted from the following sources:  \n",
    "- [Jeremy Howard's kaggle notebook: How does a neural net really work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)  \n",
    "- [fastai book Chapter 4: Under the Hood: Training a Digit Classifier](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9660b3-4c7e-4c1b-a0dd-9a5573a98500",
   "metadata": {},
   "source": [
    "## What is a neural network? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce9fa8-70f2-4cf6-9be6-24caede3719b",
   "metadata": {
    "tags": []
   },
   "source": [
    "[A neural network consists of interconnected nodes (neurons/nodes/units) which process and transmit data through a series of connections.](https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/)  \n",
    "\n",
    "The generic layered architecture of a neural network can be divided into 3 different types:  \n",
    "- input layer (receives input data and passes it on to the subsequent layer)  \n",
    "- hidden layer (does most of the computation)\n",
    "- output layer (receives output from the hidden layers, conducts the final computation and produces the final output of the network)  \n",
    "\n",
    "See below for the structure of a typical neural network - source: https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b60964-8fd5-4d15-9dfd-bf083daf4c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Structure of a typical neural network - source: https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/](https://www.deep-mind.org/wp-content/ql-cache/quicklatex.com-832dc2ccf282c799fa68d8c211646900_l3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614ac97-7254-471d-a6b5-85dd4d690860",
   "metadata": {},
   "source": [
    "### [What happens in one layer?]((https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)\n",
    "\n",
    "In the simplest of neural networks, the neural network is just a mathematical function. For one layer, the function will perform the following basic steps:  \n",
    "\n",
    "1. Multiplies each input by a number of parameters,  \n",
    "2. Adds them up for each group of parameters,  \n",
    "3. Replaces the negative numbers with zero (if using Rectified Linear Unit as the activation function)  \n",
    "\n",
    "The 3 steps are then repeated using the outputs of the previous layers as the inputs to the next layer.  \n",
    "\n",
    "The initial parameters are selected randomly, and then updated using an optimization algorithm such as stochastic gradient descent, using a loss function (to calculate how good the model is) and a predetermined learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfdbcb-7199-4c96-98c9-daea5253ff8a",
   "metadata": {},
   "source": [
    "## What is an optimization algorithm? \n",
    "\n",
    "The are many optimization algorithms available that are meant for certain problems. [There is no reason to prefer one algorithm over another unless we make assumptions about the probability distribution over the space of possible objective functions.](https://books.google.com.sg/books?hl=en&lr=&id=uBSMDwAAQBAJ&oi=fnd&pg=PR7&dq=algorithms+for+optimization&ots=sxZU0plyMF&sig=d9xWr8KEwcF1e9WEzgronSCYU4U&redir_esc=y#v=onepage&q=algorithms%20for%20optimization&f=false) For many optimization algorithms to work effectively, there needs to be some regularity in the objective function, such as Lipschitz continuity or convexity. Each algorithm would have their own assumptions, motivations for their mechanism and their advantages and disadvantages.\n",
    "\n",
    "**What is gradient descent?**\n",
    "\n",
    "Gradient descent is an [iterative first-order optimization algorithm used to find a local minimum/maximum of a given function](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21). The requirements are that the function has to be:  \n",
    "- differentiable (i.e. it has a derivative for each point in its domain)  \n",
    "- convex \n",
    "\n",
    "For more information: \n",
    "- [3 blue 1 brown: Gradient descent, how neural networks learn](https://www.3blue1brown.com/lessons/gradient-descent)\n",
    "- https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf1dce-c1ab-40f2-be57-9b5aadab708e",
   "metadata": {},
   "source": [
    "### [intuition: How can we fit a function with gradient descent?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299067c8-767c-4b52-aed2-39b9b6711af3",
   "metadata": {},
   "source": [
    "To get to the intuition behind gradient descent, let's start with a simple function - a quadratic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bb7658-df78-48ad-8b0f-7f313084adcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:58.908374Z",
     "start_time": "2023-08-28T05:49:57.695774Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "from functools import partial\n",
    "from ipywidgets import interact\n",
    "import matplotlib.figure\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b26516-dfd5-40ad-92d1-1f2f6f3657f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:58.924048Z",
     "start_time": "2023-08-28T05:49:58.910242Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_function(\n",
    "    f: Callable,\n",
    "    title: str = None,\n",
    "    min: float = -2.1,\n",
    "    max: float = 2.1,\n",
    "    color: str = 'r',\n",
    "    ylim: Tuple = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Create a simple plot of a generic function\"\"\"\n",
    "    \n",
    "    x = torch.linspace(min, max, 100)\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    plt.plot(x, f(x), color)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d4c13b-b046-487b-b471-cb9b10fee448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:58.939476Z",
     "start_time": "2023-08-28T05:49:58.925417Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_function(\n",
    "    a: float, \n",
    "    b: float, \n",
    "    c: float, \n",
    "    x: float\n",
    ")-> Callable: \n",
    "    \"\"\"Define a quadratic function\"\"\"\n",
    "    return a*x**2 + b*x + c\n",
    "    \n",
    "def make_quadratic(\n",
    "    a: float,\n",
    "    b: float,\n",
    "    c: float\n",
    ")-> Callable:\n",
    "    \"\"\"Create a generic quadratic function by passing in values for the parameters a,b,c\"\"\"\n",
    "    return partial(quadratic_function, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9286033d-b8a5-423f-9e5b-4607f8d58f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:58.955449Z",
     "start_time": "2023-08-28T05:49:58.942450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simulate some noisy data \n",
    "def noise(x, scale):\n",
    "    return np.random.normal(scale=scale, size=x.shape)\n",
    "\n",
    "def add_noise(x, multiply, add):\n",
    "    return x*(1+noise(x,multiply))+noise(x, add)\n",
    "\n",
    "np.random.seed(42)\n",
    "def f(x): return 3*x**2 + 2*x + 1 #'true' function\n",
    "x = torch.linspace(-2,2,steps=20)[:,None]\n",
    "y = add_noise(f(x), 0.15, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234db34-d263-450c-b4c3-cfc96ae6f82b",
   "metadata": {},
   "source": [
    "Here's some simulated data below. **How can we fit or find the optimal parameters for a function using gradient descent?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343e1500-f07a-402c-b38a-516f32cfe629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:59.083517Z",
     "start_time": "2023-08-28T05:49:58.957451Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the simulated data\n",
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966c290-37fa-4c4e-b31b-5e27dc299335",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before we can find the 'best' fit, we also need to quantify what we mean by model fit.   \n",
    "\n",
    "A way of quantifying this would be to measure the distance from the model/curve to the data points. To ensure that negative distances do not cancel out the positive distances, we find the absolute of this distance, and then we find the mean of all the absolute distance values.  \n",
    "\n",
    "This is known as the mean absolute error (MAE), which is mean of the absolute distance from each data point to the model/curve.  \n",
    "This is our loss function for our current use case.  \n",
    "\n",
    "The 'best' fit would be a model with parameters that have the lowest value for MAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52f1fb5-d13f-4add-aace-d6f819566aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:59.098920Z",
     "start_time": "2023-08-28T05:49:59.086436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae(\n",
    "    predicted: list, \n",
    "    true: list\n",
    ")-> float:\n",
    "    \"\"\"calculate mean absolute error based on predicted and true values\"\"\"\n",
    "    return (torch.abs(predicted-true)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d159d4-feb8-4b15-82bc-3e6091ddacd0",
   "metadata": {},
   "source": [
    "#### Manual method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6680a71-2b50-4e47-ba4f-2e744e7a1b6e",
   "metadata": {},
   "source": [
    "This is a method based on visually eye-balling the fit and the displayed mean absolute error associated with manual changes in the parameter values of the quadratic function.  \n",
    "Although this is rudimentary, it is useful in giving us a basic intuition as to what we are trying to achieve in fitting a model to a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ae329f-ab52-4064-8041-95e83588b317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:59.209814Z",
     "start_time": "2023-08-28T05:49:59.100871Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6480a4420f624d928683baa2aef85e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.1, description='a', max=3.3000000000000003, min=-1.1), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(a=1.1, b=1.1, c=1.1)\n",
    "def plot_quadratic(a,b,c):\n",
    "    f = make_quadratic(a,b,c)\n",
    "    plt.scatter(x,y)\n",
    "    loss = mae(f(x),y)\n",
    "    plot_function(\n",
    "        f,\n",
    "        ylim=(-3,13),\n",
    "        title=f\"Mean Absolute Error: {loss:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd56dc9-aae4-40bb-ab4a-34ba3e186e59",
   "metadata": {},
   "source": [
    "#### [Automating gradient descent](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314999d1-4d00-4aad-b772-3fd9bc9bdbe1",
   "metadata": {},
   "source": [
    "[Intuition](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work): if we know the gradient of our `mae()` function (loss function) *with respect to* our parameters `a`, `b`, `c`, then we have a way of adjusting the parameters. i.e. if `a` has a negative gradient, then increasing `a` will decrease `mae()`. \n",
    "\n",
    "We would first need a function that takes in the parameters `a`, `b`, `c` as a single vector input, an returns the value of `mae()` based on those parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e580d81-d55e-42d3-afc2-71c3866b0aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:59.224815Z",
     "start_time": "2023-08-28T05:49:59.211795Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_mae(params: list, y)-> float:\n",
    "    \"\"\"Return the mean absolute error for a generic quadratic function a*x^2 + b*x + c based on an input list of 3 parmeters\"\"\"\n",
    "    f = make_quadratic(*params)\n",
    "    return mae(f(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa72fc4-db7e-4546-b1ec-1caf2be9bdf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use `PyTorch` to calculate the gradients for the parameters, and repeat this 10 times to see what effect this has on the loss/ mean absolute error. \n",
    "Note that we're also using a learning rate of 0.01 to start off with.  \n",
    "- if we pick a learning rate that is too low, we would need to do a lot of steps to minimize the loss. \n",
    "\n",
    "![low learning rate](https://raw.githubusercontent.com/fastai/fastbook/823b69e00aa1e1c1a45fe88bd346f11e8f89c1ff//images/chapter2_small.svg)\n",
    "\n",
    "- on the other hand, if we pick a learning rate that is too high. it can actually result in the loss getting worse as seen in the following illustrations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fba87-6362-484c-bce7-cfa087439e08",
   "metadata": {
    "tags": []
   },
   "source": [
    "learning rate that is too high:  \n",
    "![high learning rate](https://raw.githubusercontent.com/fastai/fastbook/823b69e00aa1e1c1a45fe88bd346f11e8f89c1ff//images/chapter2_div.svg)  \n",
    "![other consequence of high learning rate](https://raw.githubusercontent.com/fastai/fastbook/823b69e00aa1e1c1a45fe88bd346f11e8f89c1ff//images/chapter2_bouncy.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c8c038e-4373-40a4-a039-1a7ccb2686ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:59.869901Z",
     "start_time": "2023-08-28T05:49:59.225795Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0; loss=7.55\n",
      "step:1; loss=7.52\n",
      "step:2; loss=7.46\n",
      "step:3; loss=7.36\n",
      "step:4; loss=7.24\n",
      "step:5; loss=7.08\n",
      "step:6; loss=6.89\n",
      "step:7; loss=6.67\n",
      "step:8; loss=6.41\n",
      "step:9; loss=6.13\n",
      "step:10; loss=5.83\n",
      "step:11; loss=5.49\n",
      "step:12; loss=5.15\n",
      "step:13; loss=4.79\n",
      "step:14; loss=4.41\n",
      "step:15; loss=4.02\n",
      "step:16; loss=3.61\n",
      "step:17; loss=3.20\n",
      "step:18; loss=2.78\n",
      "step:19; loss=2.34\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01 \n",
    "abc = torch.randn(3) # random initial parameters\n",
    "abc.requires_grad_() # tell pytorch to calculate gradients for these parameters \n",
    "\n",
    "for i in range(20):\n",
    "    loss = quadratic_mae(abc,y)\n",
    "    loss.backward() # get pytorch to calculate gradients\n",
    "    with torch.no_grad(): # dont calculate gradients for this as abc.grad*learning_rate is not part of the quadratic model \n",
    "        abc -= abc.grad*learning_rate # optimizer step\n",
    "    print(f\"step:{i}; loss={loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e86aee8-d2ab-40d3-850d-6793f98d9ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:59.885886Z",
     "start_time": "2023-08-28T05:49:59.873883Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3156, 1.8769, 1.5930], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa936e-f65e-4bf3-8cda-6c2fecb0dbdc",
   "metadata": {},
   "source": [
    "Note that in this simple example above, we did not split the dataset into a training set, validation set and test set - we just used all of the values for our simulated 'y' to calculate the loss. In a more practical setting, however, the [optimization step would involve mini-batches instead using the training dataset](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb). We'd also monitor our performance metrics (in this case the MAE of the validation set) during this process. \n",
    "\n",
    "**Why use mini-batches?**   \n",
    "Calculating the loss for the whole dataset would take a very long time, while calculating it for a single data item would result in a very imprecise and unstable gradient. The compromise is to use mini-batches;  \n",
    "- larger batch sizes would result in a more accurate and stable estimate of the dataset's gradient from the loss function,  \n",
    "- but it will take longer and we will process fewer mini-batches per epoch. \n",
    "Also, we would randomly shuffle our dataset for each epoch, before we create the mini-batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100b5b9-3c61-4e01-8d74-a5a8c7b33453",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### [Summary of steps involved in gradient descent](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)\n",
    "\n",
    "1. Initialize the parameters to random values.  \n",
    "2. Calculate the predictions.  \n",
    "3. Calculate the loss.  \n",
    "4. Calculate the gradients (i.e. an approximation of how the parameters need to change)  \n",
    "5. Step the weights (i.e. update the parameters based on the calculated gradients)  \n",
    "6. Repeat the process  \n",
    "7. Stop the epochs - watch the training and validation losses and our performance metrics to decide when to stop. \n",
    "\n",
    "So now we know how gradient descent works to help us to automatically adjust the parameters of the function based on the existing data. But how can a neural network approximate any given function? see below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebabe9-3cd2-4772-892e-394c4bbd2c92",
   "metadata": {},
   "source": [
    "### How can a neural net approximate any given function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290a7b1-8cd6-488b-8f9f-693a406be181",
   "metadata": {
    "tags": []
   },
   "source": [
    "A neural network can approximate any computable function, given enough parameters - this is implied by the Universal approximation theorem.\n",
    "\n",
    "Let's look at a simple [activation function (a functional that is applied to an intermediary output of a neuron) that allow it to model nonlinear relationships between the input and output data](https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/). \n",
    "\n",
    "#### [ReLU function](https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/)\n",
    "The rectified linear unit function maps any negative input to 0 and any positive input to itsef. The definition is as follows:\n",
    "\n",
    "![](https://www.deep-mind.org/wp-content/ql-cache/quicklatex.com-932b7b22111361835105b9ceb79c0ee1_l3.svg)  \n",
    "source: https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf15809-3529-4583-8383-d13309b08223",
   "metadata": {},
   "source": [
    "[So, how does the recified linear function work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "439bae44-d6fe-463f-b967-a262fe5569a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:49:59.981885Z",
     "start_time": "2023-08-28T05:49:59.887885Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0c1a35832543218de9e26a32f5bc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.5, description='weight', max=4.5, min=-1.5), FloatSlider(value=1.5, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rectified_linear(\n",
    "    weight: float,\n",
    "    bias: float,\n",
    "    x: list\n",
    ")-> Callable:\n",
    "    \"\"\"create rectified linear unit function\"\"\"\n",
    "    y = weight*x+bias\n",
    "    return torch.clip(y, 0.) # alternatively it can be just F.relu(weight*x+bias)\n",
    "\n",
    "@interact(weight=1.5, bias=1.5)\n",
    "def plot_relu(weight, bias):\n",
    "    plot_function(partial(rectified_linear, weight, bias),ylim=(-1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004fd5a-1cb2-4aab-b843-f45605707443",
   "metadata": {},
   "source": [
    "But the magic happens when you combine the rectified linear functions together, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56db43f1-4659-4289-b394-cdb139fd07c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:50:00.092683Z",
     "start_time": "2023-08-28T05:49:59.984887Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7ed95cf94340febcb8e2d7c2eb587c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.5, description='weight1', max=1.5, min=-4.5), FloatSlider(value=-1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def double_relu(\n",
    "    weight1:float,\n",
    "    bias1:float,\n",
    "    weight2:float,\n",
    "    bias2: float,\n",
    "    x\n",
    ")-> Callable:\n",
    "    return rectified_linear(weight1, bias1,x) + rectified_linear(weight2, bias2, x)\n",
    "\n",
    "@interact(weight1=-1.5, bias1=-1.5, weight2=1.5, bias2=1.5)\n",
    "def plot_double_relu(weight1, bias1, weight2, bias2):\n",
    "    plot_function(partial(double_relu, weight1, bias1, weight2, bias2), ylim=(-1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef672ed-b7de-4aad-9310-596e7d72cd38",
   "metadata": {},
   "source": [
    "Let's try a triple relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6025e6a0-ac79-4898-9d32-6b2531bb68df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:50:00.202760Z",
     "start_time": "2023-08-28T05:50:00.094683Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b30bdddf0d4442a99f5bcde0249222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.5, description='weight1', max=1.5, min=-4.5), FloatSlider(value=-1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def triple_relu(\n",
    "    weight1:float,\n",
    "    bias1:float,\n",
    "    weight2:float,\n",
    "    bias2: float,\n",
    "    weight3: float,\n",
    "    bias3: float,\n",
    "    x\n",
    ")-> Callable:\n",
    "    return rectified_linear(weight1, bias1,x) + rectified_linear(weight2, bias2, x) + rectified_linear(weight3, bias3, x)\n",
    "\n",
    "@interact(weight1=-1.5, bias1=-1.5, weight2=1.5, bias2=1.5, weight3=1.5, bias3=1.5)\n",
    "def plot_double_relu(weight1, bias1, weight2, bias2, weight3, bias3):\n",
    "    plot_function(partial(triple_relu, weight1, bias1, weight2, bias2, weight3, bias3), ylim=(-1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824f61e-07ab-413a-b447-24f72ee3585e",
   "metadata": {},
   "source": [
    "[With enough of these rectified linear functions added together, you could approximate any function with a single input to whatevery accuracy you like.](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69798512-e6ca-4c21-98b3-a99da8a134f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Simple net from scratch](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)\n",
    "\n",
    "In this very basic neural network, we have a non-linear function (ReLU function) between 2 linear models (without an optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5303068-1502-400c-9950-74e66b8ad0e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T05:50:00.217995Z",
     "start_time": "2023-08-28T05:50:00.204683Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_net(xb:torch.FloatTensor)-> Callable:\n",
    "    \"\"\"basic neural net\"\"\"\n",
    "    res = xb@weight1 + bias1 # linear model\n",
    "    res = res.max(tensor(0.0)) # rectified linear unit \n",
    "    res = res@weight2 + bias2\n",
    "    setattr(res, \"params\", {'weight1':weight1, 'bias1':bias1, 'weight2':weight2, 'bias2':bias2})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d46ce0-fc02-47b1-94c0-dbb39023121d",
   "metadata": {
    "tags": []
   },
   "source": [
    "`@`: matrix multiplication   \n",
    "\n",
    "the dot product betwen the first matrix row and the the second matrix column can be visualized as below:  \n",
    "![](https://raw.githubusercontent.com/fastai/fastbook/823b69e00aa1e1c1a45fe88bd346f11e8f89c1ff//images/matmul2.svg)  \n",
    "source: https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb  \n",
    "\n",
    "or from [3 Blue 1 Brown: Matrix Multiplication](https://www.3blue1brown.com/lessons/matrix-multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfda87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
